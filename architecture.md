Step 1: Evaluate and compare the classification accuracy, recall, precision, F1-score, and AUC achieved by Single Network, Monte Carlo Dropout (MCD), and Deep Ensemble (DE) on the three datasets 
Step 2: Analyze each model's calibration performance (e.g., via ECE, Brier score), identifying which base approach is best calibrated across the three datasets 
Step 3: For each base method's (SingleNet, MCD, DE), apply APS, RAPS, and SAPS to create conformal sets, then compare their mean/median set sizes, marginal coverage, etc... and accuracy-per-set-size trade-offs to come out with the most efficient combinaison strategy on the three datasets. 
Step 4: Investigate how predictive entropy correlates with classification performance for each method, quantify effect sizes (using KS tests and Cohen's d), and visualize rejection plots to see which approach exhibits the strongest relationship between uncertainty and accuracy 
Step 5: Investigate the correlation between predictive entropy and predictive set size by using the scatter plot, Kruskal Wallis test and spearman rank correlation coefficient to identify among SingleNet, MCD and DE the ones which produce the more aligned uncertainty measure 
Step 6: Assess how effectively each base method flags OOD samples, comparing them using standard OOD metrics such as AUROC and AUPRC 
Step 7: In collaboration with a medical specialist, qualitatively and quantitatively evaluate saliency or attention maps produced by each UQ method, determining which yields the most clinically interpretable explanations 
Step 8: Implement a demonstration system showcasing the best-performing hybrid framework (from the tested base UQ + CP method combinations), presenting each prediction's top label, conformal prediction set, the model's uncertainty score, and an explainability map to support clinical decision-making