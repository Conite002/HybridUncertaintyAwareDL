{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# Define transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing im_Dyskeratotic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2439/2439 [00:00<00:00, 17773.68it/s]\n",
      "Processing im_Koilocytotic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2475/2475 [00:00<00:00, 11059.10it/s]\n",
      "Processing im_Metaplastic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2379/2379 [00:00<00:00, 9142.08it/s]\n",
      "Processing im_Parabasal: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2361/2361 [00:00<00:00, 14176.74it/s]\n",
      "Processing im_Superficial-Intermediate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2493/2493 [00:00<00:00, 5162.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Organisation et renommage des fichiers termin√©s ! ‚úÖ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "base_dir = \"../Datasets/SIPaKMeD\"\n",
    "output_dir = \"../Outputs/sipakmed_combine\"\n",
    "folders = [\"im_Dyskeratotic\", \"im_Koilocytotic\", \"im_Metaplastic\", \"im_Parabasal\", \"im_Superficial-Intermediate\"]\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "label_mapping = {folder: i for i, folder in enumerate(folders)}\n",
    "\n",
    "for folder in folders:\n",
    "    class_dir = os.path.join(base_dir, folder, \"CROPPED\")\n",
    "    \n",
    "    if os.path.exists(class_dir):\n",
    "        for file in tqdm(os.listdir(class_dir), desc=f\"Processing {folder}\"):\n",
    "            if file.endswith(\".bmp\"): \n",
    "                old_path = os.path.join(class_dir, file)\n",
    "                new_filename = f\"{label_mapping[folder]}__{file.split('.')[0]}.bmp\"\n",
    "                new_path = os.path.join(output_dir, new_filename)\n",
    "                shutil.copy(old_path, new_path)\n",
    "\n",
    "print(\"Organisation et renommage des fichiers termin√©s ! ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 4049\n",
      "Total labels: 4049\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_folder = \"../Outputs/sipakmed_combine\"\n",
    "\n",
    "image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith('.bmp')]\n",
    "labels = [int(img.split('__')[0]) for img in os.listdir(image_folder) if img.endswith('.bmp')]  # Extraction du label\n",
    "\n",
    "print(f\"Total images: {len(image_paths)}\")\n",
    "print(f\"Total labels: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3036, Cal: 455, Val: 152, Test: 406\n"
     ]
    }
   ],
   "source": [
    "split_ratios = {\"train\": 0.75, \"cal\": 0.1125, \"val\": 0.0375, \"test\": 0.10}\n",
    "\n",
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    image_paths, labels, test_size=1 - split_ratios[\"train\"], stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "remaining_ratio = 1 - split_ratios[\"train\"]\n",
    "cal_ratio = split_ratios[\"cal\"] / remaining_ratio\n",
    "val_ratio = split_ratios[\"val\"] / remaining_ratio\n",
    "test_ratio = split_ratios[\"test\"] / remaining_ratio\n",
    "\n",
    "cal_paths, temp_paths, cal_labels, temp_labels = train_test_split(\n",
    "    temp_paths, temp_labels, test_size=(val_ratio + test_ratio), stratify=temp_labels, random_state=42\n",
    ")\n",
    "\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths, temp_labels, test_size=(test_ratio / (val_ratio + test_ratio)), stratify=temp_labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_paths)}, Cal: {len(cal_paths)}, Val: {len(val_paths)}, Test: {len(test_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_train = ImageDataset(train_paths, train_labels, transform=transform)\n",
    "dataset_val = ImageDataset(val_paths, val_labels, transform=transform)\n",
    "dataset_cal = ImageDataset(cal_paths, cal_labels, transform=transform)\n",
    "dataset_test = ImageDataset(test_paths, test_labels, transform=transform)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=32, shuffle=False)\n",
    "dataloader_cal = DataLoader(dataset_cal, batch_size=32, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [00:30<00:00,  3.07it/s]\n",
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  3.19it/s]\n",
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:04<00:00,  3.42it/s]\n",
      "Extracting features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:04<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load ResNet50 model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet.fc = nn.Identity() \n",
    "resnet.eval()\n",
    "resnet.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(dataloader):\n",
    "    features, labels_list = [], []\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            images = images.to(device)\n",
    "            feat = resnet(images)\n",
    "            features.append(feat.cpu().numpy())\n",
    "            labels_list.extend(labels.numpy())\n",
    "    return np.vstack(features), np.array(labels_list)\n",
    "\n",
    "# Extract features for train, val, and test\n",
    "features_train, labels_train = extract_features(dataloader_train)\n",
    "features_val, labels_val = extract_features(dataloader_val)\n",
    "features_cal, labels_cal = extract_features(dataloader_cal)\n",
    "features_test, labels_test = extract_features(dataloader_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"../Feature_Extraction/sipakmed_features.npz\", train=features_train, train_labels=labels_train, \n",
    "         val=features_val, val_labels=labels_val,\n",
    "         cal=features_cal, cal_labels=labels_cal,\n",
    "         test=features_test, test_labels=labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility across different libraries:\n",
    "    numpy, torch, and cuda if available.\n",
    "    \n",
    "    Args:\n",
    "        seed (int): The seed value for random number generators. Default is 42.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 46.8372, Val Loss: 1.3598, Val Acc: 0.8816\n",
      "Epoch 2/25, Loss: 21.4198, Val Loss: 1.2374, Val Acc: 0.9145\n",
      "Epoch 3/25, Loss: 17.9827, Val Loss: 1.0701, Val Acc: 0.9408\n",
      "Epoch 4/25, Loss: 17.7786, Val Loss: 0.9791, Val Acc: 0.9342\n",
      "Epoch 5/25, Loss: 14.4647, Val Loss: 1.2757, Val Acc: 0.9013\n",
      "Epoch 6/25, Loss: 11.7573, Val Loss: 1.3027, Val Acc: 0.8882\n",
      "Epoch 7/25, Loss: 10.1886, Val Loss: 1.0554, Val Acc: 0.9342\n",
      "Epoch 8/25, Loss: 9.1647, Val Loss: 0.9319, Val Acc: 0.9408\n",
      "Epoch 9/25, Loss: 7.8848, Val Loss: 1.5678, Val Acc: 0.9276\n",
      "Epoch 10/25, Loss: 8.9252, Val Loss: 1.3154, Val Acc: 0.9342\n",
      "Epoch 11/25, Loss: 8.4252, Val Loss: 1.1965, Val Acc: 0.9539\n",
      "Epoch 12/25, Loss: 8.7049, Val Loss: 0.9013, Val Acc: 0.9539\n",
      "Epoch 13/25, Loss: 8.4871, Val Loss: 1.0519, Val Acc: 0.9408\n",
      "Epoch 14/25, Loss: 7.7927, Val Loss: 1.0113, Val Acc: 0.9408\n",
      "Epoch 15/25, Loss: 7.0666, Val Loss: 0.9268, Val Acc: 0.9408\n",
      "Epoch 16/25, Loss: 6.5883, Val Loss: 1.2564, Val Acc: 0.9276\n",
      "Epoch 17/25, Loss: 6.0760, Val Loss: 1.1705, Val Acc: 0.9474\n",
      "Epoch 18/25, Loss: 4.2019, Val Loss: 1.3263, Val Acc: 0.9342\n",
      "Epoch 19/25, Loss: 2.2019, Val Loss: 1.3171, Val Acc: 0.9539\n",
      "Epoch 20/25, Loss: 1.3302, Val Loss: 1.4148, Val Acc: 0.9539\n",
      "Epoch 21/25, Loss: 0.9133, Val Loss: 1.3040, Val Acc: 0.9539\n",
      "Epoch 22/25, Loss: 0.7093, Val Loss: 1.2045, Val Acc: 0.9539\n",
      "Epoch 23/25, Loss: 1.7072, Val Loss: 1.4730, Val Acc: 0.9474\n",
      "Epoch 24/25, Loss: 1.6834, Val Loss: 1.0232, Val Acc: 0.9474\n",
      "Epoch 25/25, Loss: 3.3700, Val Loss: 1.0180, Val Acc: 0.9408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_483584/1456636576.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mlp.load_state_dict(torch.load(\"best_mlp_classifier.pth\"))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the best hyperparameters for the MLP model using Optuna**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Training MLP with Batch: 32, Epochs: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:  11%|‚ñà         | 1/9 [00:00<00:06,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 32, Epochs Trained 25, Best Validation Loss: 0.7894\n",
      "\n",
      "üöÄ Training MLP with Batch: 32, Epochs: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:  22%|‚ñà‚ñà‚ñè       | 2/9 [00:02<00:07,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 32, Epochs Trained 50, Best Validation Loss: 0.3579\n",
      "\n",
      "üöÄ Training MLP with Batch: 32, Epochs: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:  33%|‚ñà‚ñà‚ñà‚ñé      | 3/9 [00:04<00:11,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 32, Epochs Trained 100, Best Validation Loss: 0.2401\n",
      "\n",
      "üöÄ Training MLP with Batch: 64, Epochs: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 4/9 [00:05<00:06,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 64, Epochs Trained 25, Best Validation Loss: 0.7872\n",
      "\n",
      "üöÄ Training MLP with Batch: 64, Epochs: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 5/9 [00:07<00:05,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 64, Epochs Trained 50, Best Validation Loss: 0.3555\n",
      "\n",
      "üöÄ Training MLP with Batch: 64, Epochs: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 6/9 [00:09<00:05,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 64, Epochs Trained 100, Best Validation Loss: 0.2451\n",
      "\n",
      "üöÄ Training MLP with Batch: 128, Epochs: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 7/9 [00:10<00:03,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 128, Epochs Trained 25, Best Validation Loss: 0.7270\n",
      "\n",
      "üöÄ Training MLP with Batch: 128, Epochs: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 8/9 [00:12<00:01,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 128, Epochs Trained 50, Best Validation Loss: 0.3372\n",
      "\n",
      "üöÄ Training MLP with Batch: 128, Epochs: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparameter Tuning: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:15<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Batch 128, Epochs Trained 100, Best Validation Loss: 0.2451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load extracted features\n",
    "data = np.load(\"../Feature_Extraction/sipakmed_features.npz\")\n",
    "features_train, labels_train = data[\"train\"], data[\"train_labels\"]\n",
    "features_val, labels_val = data[\"val\"], data[\"val_labels\"]\n",
    "features_test, labels_test = data[\"test\"], data[\"test_labels\"]\n",
    "\n",
    "# Define MLP classifier\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1024)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters to test\n",
    "BATCHS = [32, 64, 128]\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = [25, 50, 100]\n",
    "PATIENCE = 5\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "best_model = None\n",
    "best_config = None\n",
    "lowest_val_loss = float(\"inf\")\n",
    "\n",
    "# Device setup\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Iterate over all combinations of batch_size and num_epochs\n",
    "for batch_size, num_epochs in tqdm(itertools.product(BATCHS, NUM_EPOCHS), total=len(BATCHS) * len(NUM_EPOCHS), desc=\"Hyperparameter Tuning\"):\n",
    "\n",
    "    print(f\"\\nüöÄ Training MLP with Batch: {batch_size}, Epochs: {num_epochs}\")\n",
    "\n",
    "    # Create model\n",
    "    input_dim = features_train.shape[1]\n",
    "    num_classes = len(set(labels_train))\n",
    "    mlp = MLPClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(mlp.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Convert features to tensors\n",
    "    X_train = torch.tensor(features_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(labels_train, dtype=torch.long).to(device)\n",
    "    X_val = torch.tensor(features_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(labels_val, dtype=torch.long).to(device)\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training with tqdm and patience\n",
    "    for epoch in tqdm(range(num_epochs), desc=f\"Training Epochs (Batch {batch_size})\", leave=False):\n",
    "        mlp.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        mlp.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = mlp(X_val)\n",
    "            val_loss = criterion(val_outputs, y_val).item()  # Compute loss\n",
    "            val_preds = val_outputs.argmax(1).cpu().numpy()\n",
    "            val_acc = accuracy_score(labels_val, val_preds)\n",
    "\n",
    "        # Check if validation loss improved\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = mlp.state_dict()\n",
    "            epochs_no_improve = 0  # Reset counter\n",
    "        else:\n",
    "            epochs_no_improve += 1  # Increase counter\n",
    "\n",
    "        # Early stopping condition\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"‚èπÔ∏è Early stopping triggered after {epoch+1} epochs (Best Val Loss: {best_val_loss:.4f})\")\n",
    "            break\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs_trained\": epoch+1,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "        \"best_val_acc\": val_acc\n",
    "    })\n",
    "\n",
    "    if best_val_loss < lowest_val_loss:\n",
    "        lowest_val_loss = best_val_loss\n",
    "        best_model = best_model_state\n",
    "        best_config = {\"batch_size\": batch_size, \"epochs\": epoch+1, \"val_loss\": best_val_loss, \"val_acc\": val_acc}\n",
    "\n",
    "    print(f\"‚úÖ Batch {batch_size}, Epochs Trained {epoch+1}, Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_model, \"best_mlp_model.pth\")\n",
    "\n",
    "# Display results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ Best Model Configuration:\n",
      "{'batch_size': 32, 'epochs': 100, 'val_loss': 0.2400825470685959, 'val_acc': 0.9144736842105263}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nüèÜ Best Model Configuration:\")\n",
    "print(best_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use best hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 91.7326, Val Loss: 2.5975, Val Acc: 0.8224\n",
      "Validation loss decreased to 2.5975. Saving model...\n",
      "Epoch 2/100, Loss: 37.7243, Val Loss: 1.7129, Val Acc: 0.8816\n",
      "Validation loss decreased to 1.7129. Saving model...\n",
      "Epoch 3/100, Loss: 27.3474, Val Loss: 1.4699, Val Acc: 0.8947\n",
      "Validation loss decreased to 1.4699. Saving model...\n",
      "Epoch 4/100, Loss: 22.6385, Val Loss: 1.3914, Val Acc: 0.8947\n",
      "Validation loss decreased to 1.3914. Saving model...\n",
      "Epoch 5/100, Loss: 19.8084, Val Loss: 1.3564, Val Acc: 0.9013\n",
      "Validation loss decreased to 1.3564. Saving model...\n",
      "Epoch 6/100, Loss: 17.7172, Val Loss: 1.3313, Val Acc: 0.9079\n",
      "Validation loss decreased to 1.3313. Saving model...\n",
      "Epoch 7/100, Loss: 16.0306, Val Loss: 1.3138, Val Acc: 0.9079\n",
      "Validation loss decreased to 1.3138. Saving model...\n",
      "Epoch 8/100, Loss: 14.6591, Val Loss: 1.3052, Val Acc: 0.9079\n",
      "Validation loss decreased to 1.3052. Saving model...\n",
      "Epoch 9/100, Loss: 13.4559, Val Loss: 1.2996, Val Acc: 0.9145\n",
      "Validation loss decreased to 1.2996. Saving model...\n",
      "Epoch 10/100, Loss: 12.4063, Val Loss: 1.3027, Val Acc: 0.9145\n",
      "No improvement for 1 epochs\n",
      "Epoch 11/100, Loss: 11.5050, Val Loss: 1.3114, Val Acc: 0.9145\n",
      "No improvement for 2 epochs\n",
      "Epoch 12/100, Loss: 10.7188, Val Loss: 1.3160, Val Acc: 0.9145\n",
      "No improvement for 3 epochs\n",
      "Epoch 13/100, Loss: 10.0474, Val Loss: 1.3293, Val Acc: 0.9145\n",
      "No improvement for 4 epochs\n",
      "Epoch 14/100, Loss: 9.3668, Val Loss: 1.3534, Val Acc: 0.9145\n",
      "No improvement for 5 epochs\n",
      "Epoch 15/100, Loss: 8.7876, Val Loss: 1.3805, Val Acc: 0.9145\n",
      "No improvement for 6 epochs\n",
      "Epoch 16/100, Loss: 8.2062, Val Loss: 1.4205, Val Acc: 0.9145\n",
      "No improvement for 7 epochs\n",
      "Epoch 17/100, Loss: 7.6189, Val Loss: 1.4719, Val Acc: 0.9145\n",
      "No improvement for 8 epochs\n",
      "Epoch 18/100, Loss: 7.0496, Val Loss: 1.5264, Val Acc: 0.9079\n",
      "No improvement for 9 epochs\n",
      "Epoch 19/100, Loss: 6.6830, Val Loss: 1.5973, Val Acc: 0.9079\n",
      "No improvement for 10 epochs\n",
      "Early stopping triggered after 19 epochs\n",
      "Loading best model for testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_483584/2136972158.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  mlp.load_state_dict(torch.load(\"best_mlp_classifier.pth\"))\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Training the MLP\n",
    "input_dim = features_train.shape[1]\n",
    "num_classes = len(set(labels_train))\n",
    "mlp = MLPClassifier(input_dim, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "\n",
    "def get_batches(X, y, batch_size):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield torch.tensor(X[i:i+batch_size], dtype=torch.float32), torch.tensor(y[i:i+batch_size], dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 10 \n",
    "batch_size = 32 \n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    mlp.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in get_batches(features_train, labels_train, batch_size=batch_size):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    mlp.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in get_batches(features_val, labels_val, batch_size=batch_size):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = mlp(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "            val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            val_targets.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    val_accuracy = accuracy_score(val_targets, val_preds)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Check if validation loss improved\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        print(f\"Validation loss decreased to {val_loss:.4f}. Saving model...\")\n",
    "        torch.save(mlp.state_dict(), \"best_mlp_classifier.pth\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epochs\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "# Load the best model for testing\n",
    "print(\"Loading best model for testing...\")\n",
    "mlp.load_state_dict(torch.load(\"best_mlp_classifier.pth\"))\n",
    "mlp.eval()\n",
    "test_preds, test_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in get_batches(features_test, labels_test, batch_size=batch_size):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = mlp(X_batch)\n",
    "        test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "        test_targets.extend(y_batch.cpu().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 93.1034\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = accuracy_score(test_targets, test_preds)\n",
    "print(f\"Test Accuracy: {test_accuracy*100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
